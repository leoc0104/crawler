# Python Crawler Project

This is a Python *crawler* project with MVC architecture that extracts and manipulates data in a MongoDB collection.

## Features

- **Crawler**: Extracts data from defined sources.
- **NER (Named Entity Recognition)**: Processes the extracted content to identify and store relevant entities in MongoDB.
- **CRUD**: Full manipulation of the extracted data, including creation, reading, updating, and deletion.
- **MVC Architecture**: Clear separation of responsibilities between Models, Views, and Controllers.
- **Configuration via `.env` file**: Allows setting environment variables for database configuration and other credentials.

## Technologies Used

- **Python**
- **Flask**: For creating API and web interface.
- **MongoDB**: Storage for the extracted and processed data.
- **JavaScript**: For client-side interactions.
- **HTML and CSS**: Basic interface for data visualization.

## Project Structure

```
ðŸ“‚ crawler-python
 â”£ ðŸ“‚ Controllers        # Contains the logic for data manipulation and control flow
 â”ƒ â”— ðŸ“„ EntityController.py   # Manages CRUD operations for entities
 â”£ ðŸ“‚ Models             # Contains data structure and interactions with MongoDB
 â”ƒ â”— ðŸ“„ Entity.py             # Model representing entities and their interactions with MongoDB
 â”£ ðŸ“‚ Services           # Contains scripts for data collection and processing
 â”ƒ â”£ ðŸ“„ crawler.py           # Crawler script for data extraction
 â”ƒ â”— ðŸ“„ ner.py               # NER script to identify entities in the extracted content
 â”£ ðŸ“‚ views              # Contains static files for the frontend
 â”ƒ â”£ ðŸ“„ app.js               # JavaScript for frontend interactions
 â”ƒ â”£ ðŸ“„ index.html           # Main HTML page for data visualization
 â”ƒ â”— ðŸ“„ style.css            # CSS for interface styling
 â”£ ðŸ“„ .env               # Environment variables configuration file
 â”£ ðŸ“„ .env.example       # Example .env file
 â”£ ðŸ“„ .gitignore         # File to specify ignored files by Git
 â”£ ðŸ“„ output.txt         # Example output generated by the crawler
 â”£ ðŸ“„ README.md          # Project documentation
 â”£ ðŸ“„ requirements.txt   # Project dependencies
 â”— ðŸ“„ routes.py          # Defines routes and maps them to corresponding controllers
```

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/leoc0104/crawler.git
   ```
2. Navigate to the project directory:
   ```bash
   cd crawler-python
   ```
3. Install dependencies:
   ```bash
   sudo apt install python3 python3-pip
   pip install -r requirements.txt
   ```
4. Rename `.env.example` to `.env` (or clone) and configure the environment variables.

## Running the Project

1. Run the crawler:
   ```bash
   python3 Services/crawler.py
   ```
2. Start the Flask server:
   ```bash
   python3 routes.py
   ```
3. Start the web server:
   ```bash
   python3 -m http.server 8000
   ```

## Database Structure (MongoDB)

The `entities` collection stores data extracted by the *crawler* and processed by the NER. Each document has the following fields:

- **_id**: Unique identifier.
- **name**: Name of the entity.
- **label**: Label of the extracted entity.