# Python Crawler Project

Este Ã© um projeto de *crawler* em Python com arquitetura MVC, que extrai e manipula dados em uma collection do MongoDB.

## Funcionalidades

- **Crawler**: Extrai dados de fontes definidas.
- **NER (Named Entity Recognition)**: Processa o conteÃºdo extraÃ­do para identificar e armazenar entidades relevantes no MongoDB.
- **CRUD**: ManipulaÃ§Ã£o completa dos dados extraÃ­dos, incluindo criaÃ§Ã£o, leitura, atualizaÃ§Ã£o e exclusÃ£o.
- **Arquitetura MVC**: SeparaÃ§Ã£o clara de responsabilidades entre Models, Views e Controllers.
- **ConfiguraÃ§Ã£o via arquivo `.env`**: Permite definir variÃ¡veis de ambiente para configuraÃ§Ã£o de banco de dados e outras credenciais.

## Tecnologias Utilizadas

- **Python**
- **Flask**: Para criaÃ§Ã£o de API e interface web.
- **MongoDB**: Armazenamento dos dados extraÃ­dos e processados.
- **JavaScript**: Para interaÃ§Ãµes no lado do cliente.
- **HTML e CSS**: Interface bÃ¡sica para visualizaÃ§Ã£o dos dados.

## Estrutura do Projeto

```
ğŸ“‚ crawler-python
 â”£ ğŸ“‚ Controllers        # ContÃ©m a lÃ³gica para manipulaÃ§Ã£o de dados e fluxo de controle
 â”ƒ â”— ğŸ“„ EntityController.py   # Gerencia as operaÃ§Ãµes CRUD para entidades
 â”£ ğŸ“‚ Models             # ContÃ©m a estrutura de dados e interaÃ§Ãµes com o MongoDB
 â”ƒ â”— ğŸ“„ Entity.py             # Modelo que representa as entidades e suas interaÃ§Ãµes com o MongoDB
 â”£ ğŸ“‚ Services           # ContÃ©m scripts para coleta e processamento de dados
 â”ƒ â”£ ğŸ“„ crawler.py           # Script de crawler para extrair dados
 â”ƒ â”— ğŸ“„ ner.py               # Script de NER para identificar entidades no conteÃºdo extraÃ­do
 â”£ ğŸ“‚ views              # ContÃ©m arquivos estÃ¡ticos para o frontend
 â”ƒ â”£ ğŸ“„ app.js               # JavaScript para interaÃ§Ãµes no frontend
 â”ƒ â”£ ğŸ“„ index.html           # PÃ¡gina HTML principal para visualizaÃ§Ã£o dos dados
 â”ƒ â”— ğŸ“„ style.css            # CSS para estilizaÃ§Ã£o da interface
 â”£ ğŸ“„ .env               # Arquivo de variÃ¡veis de ambiente para configuraÃ§Ã£o
 â”£ ğŸ“„ .env.example       # Exemplo de arquivo .env
 â”£ ğŸ“„ .gitignore         # Arquivo para especificar arquivos ignorados pelo Git
 â”£ ğŸ“„ output.txt         # Exemplo de saÃ­da gerada pelo crawler
 â”£ ğŸ“„ README.md          # DocumentaÃ§Ã£o do projeto
 â”£ ğŸ“„ requirements.txt   # DependÃªncias do projeto
 â”— ğŸ“„ routes.py          # Define as rotas e mapeia para os controllers correspondentes
```

## InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
   ```bash
   git clone https://github.com/leoc0104/crawler.git
   ```
2. Entre no diretÃ³rio do projeto:
   ```bash
   cd crawler-python
   ```
3. Instale as dependÃªncias:
   ```bash
   sudo apt install python3 python3-pip
   pip install -r requirements.txt
   ```
4. Renomeie o arquivo `.env.example` para `.env` (ou clone) e configure as variÃ¡veis de ambiente.

## Executando o Projeto

1. Execute o crawler:
   ```bash
   python3 Services/crawler.py
   ```
2. Inicie o servidor Flask:
   ```bash
   python3 routes.py
   ```
3. Inicie o servidor web:
   ```bash
   python3 -m http.server 8000
   ```

## Estrutura do Banco de Dados (MongoDB)

A coleÃ§Ã£o `entities` armazena os dados extraÃ­dos pelo *crawler* e processados pelo NER. Cada documento possui os seguintes campos:

- **_id**: Identificador Ãºnico.
- **name**: Nome da entidade.
- **label**: RÃ³tulo da entidade extraÃ­da.
